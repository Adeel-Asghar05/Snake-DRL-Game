{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pygame\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "# Constants\n",
    "WIDTH, HEIGHT = 600, 400\n",
    "BLOCK_SIZE = 20\n",
    "SNAKE_COLOR = (0, 255, 0)\n",
    "FOOD_COLOR = (255, 0, 0)\n",
    "BACKGROUND_COLOR = (0, 0, 0)\n",
    "FPS = 12\n",
    "\n",
    "# Hyperparameters\n",
    "GAMMA = 0.9\n",
    "EPSILON = 0.05 # Start with lower exploration\n",
    "EPSILON_MIN = 0.01  # Reduced minimum exploration rate\n",
    "EPSILON_DECAY = 0.99  # Faster decay rate for epsilon\n",
    "LEARNING_RATE = 0.001\n",
    "MEMORY_SIZE = 10000\n",
    "BATCH_SIZE = 128\n",
    "TARGET_UPDATE_FREQUENCY = 10\n",
    "\n",
    "# PyTorch device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Updated DQN network with 8 hidden layers\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc4 = nn.Linear(128, 128)\n",
    "        self.fc5 = nn.Linear(128, 128)\n",
    "        self.fc6 = nn.Linear(128, 128)\n",
    "        self.fc7 = nn.Linear(128, 128)\n",
    "        self.fc8 = nn.Linear(128, 128)\n",
    "        self.fc9 = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        x = torch.relu(self.fc5(x))\n",
    "        x = torch.relu(self.fc6(x))\n",
    "        x = torch.relu(self.fc7(x))\n",
    "        x = torch.relu(self.fc8(x))\n",
    "        return self.fc9(x)\n",
    "\n",
    "# Initialize DQNs\n",
    "policy_net = DQN(input_size=4, output_size=4).to(device)\n",
    "target_net = DQN(input_size=4, output_size=4).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# Load pre-trained model if available\n",
    "try:\n",
    "    policy_net.load_state_dict(torch.load(\"snake.pt\", map_location=device))\n",
    "    policy_net.eval() \n",
    "    print(\"evaluation mode\")\n",
    "    # policy_net.train() \n",
    "    # print(\"training mode\")\n",
    "    print(\"Loaded pre-trained model 'snake.pt'\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"No pre-trained model found, training from scratch.\")\n",
    "\n",
    "# Rest of the code remains unchanged (Optimizer, game loop, helper functions, etc.)\n",
    "\n",
    "# Optimizer and replay memory\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "memory = deque(maxlen=MEMORY_SIZE)\n",
    "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# Initialize pygame\n",
    "pygame.init()\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"DQN Snake Game-new\")\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "# Action mappings: 0 - Up, 1 - Left, 2 - Right, 3 - Down\n",
    "ACTIONS = [(0, -BLOCK_SIZE), (-BLOCK_SIZE, 0), (BLOCK_SIZE, 0), (0, BLOCK_SIZE)]\n",
    "# Modify the step method to ensure that next_state is not None\n",
    "class SnakeGame:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.snake = [(WIDTH // 2, HEIGHT // 2)]\n",
    "        self.direction = (0, -BLOCK_SIZE)\n",
    "        self.food = self.spawn_food()\n",
    "        self.score = 0\n",
    "        self.done = False\n",
    "        self.start_distance = self.calculate_distance(self.snake[0], self.food)  # Distance to food at start\n",
    "        return self.get_state()\n",
    "\n",
    "    def spawn_food(self):\n",
    "        while True:\n",
    "            x = random.randint(0, (WIDTH - BLOCK_SIZE) // BLOCK_SIZE) * BLOCK_SIZE\n",
    "            y = random.randint(0, (HEIGHT - BLOCK_SIZE) // BLOCK_SIZE) * BLOCK_SIZE\n",
    "            if (x, y) not in self.snake:\n",
    "                return (x, y)\n",
    "\n",
    "    def get_state(self):\n",
    "        head_x, head_y = self.snake[0]\n",
    "        food_x, food_y = self.food\n",
    "        state = np.array([head_x - food_x, head_y - food_y, self.direction[0], self.direction[1]], dtype=np.float32)\n",
    "        return state\n",
    "\n",
    "    def calculate_distance(self, head, food):\n",
    "        return np.abs(head[0] - food[0]) + np.abs(head[1] - food[1])\n",
    "\n",
    "    def step(self, action):\n",
    "        # Check for the new direction based on the chosen action\n",
    "        new_direction = ACTIONS[action]\n",
    "        potential_new_head = (self.snake[0][0] + new_direction[0], self.snake[0][1] + new_direction[1])\n",
    "\n",
    "    # Check if the new direction would lead to self-collision\n",
    "        if potential_new_head in self.snake:\n",
    "        # Find all possible safe directions that avoid self-collisions\n",
    "            safe_directions = [\n",
    "                d for d in ACTIONS\n",
    "                if (self.snake[0][0] + d[0], self.snake[0][1] + d[1]) not in self.snake\n",
    "            ]\n",
    "        \n",
    "        # If there are safe directions, choose one randomly\n",
    "            if safe_directions:\n",
    "                new_direction = random.choice(safe_directions)\n",
    "            else:\n",
    "                # If no safe direction is found, maintain the current direction\n",
    "                new_direction = self.direction\n",
    "\n",
    "    # Update the snake's direction\n",
    "        self.direction = new_direction\n",
    "        head_x, head_y = self.snake[0]\n",
    "        new_head = (head_x + self.direction[0], head_y + self.direction[1])\n",
    "\n",
    "    # Check for final wall or self-collision after the move\n",
    "        if (new_head in self.snake or\n",
    "                new_head[0] < 0 or new_head[0] >= WIDTH or\n",
    "                new_head[1] < 0 or new_head[1] >= HEIGHT):\n",
    "            self.done = True\n",
    "            return self.get_state(), -15, self.done  # High penalty for collision\n",
    "\n",
    "    # Move the snake to the new position\n",
    "        self.snake.insert(0, new_head)\n",
    "        reward = 0\n",
    "\n",
    "    # Calculate the distance to the food before and after the move\n",
    "        old_distance = self.start_distance\n",
    "        new_distance = self.calculate_distance(new_head, self.food)\n",
    "\n",
    "        if new_distance < old_distance:\n",
    "            reward = 1  # Reward for getting closer to food\n",
    "        elif new_distance > old_distance:\n",
    "            reward = -1  # Penalty for moving farther from food\n",
    "\n",
    "    # Check if the snake has found food\n",
    "        if new_head == self.food:\n",
    "            self.score += 1\n",
    "            reward += 10  # Reward for food\n",
    "            self.food = self.spawn_food()  # Spawn new food after eating\n",
    "            self.start_distance = self.calculate_distance(new_head, self.food)  # Reset the distance to new food\n",
    "\n",
    "        else:\n",
    "            self.snake.pop()  # Remove the last segment if no food is eaten\n",
    "\n",
    "        next_state = self.get_state()  # Always return the next state, even when done\n",
    "        return next_state, reward, self.done\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        screen.fill(BACKGROUND_COLOR)\n",
    "        for segment in self.snake:\n",
    "            pygame.draw.rect(screen, SNAKE_COLOR, (*segment, BLOCK_SIZE, BLOCK_SIZE))\n",
    "        pygame.draw.rect(screen, FOOD_COLOR, (*self.food, BLOCK_SIZE, BLOCK_SIZE))\n",
    "        font = pygame.font.Font(None, 36)\n",
    "        score_text = font.render(f'Score: {self.score}', True, (255, 255, 255))\n",
    "        screen.blit(score_text, (10, 10))\n",
    "        pygame.display.flip()\n",
    "\n",
    "# Helper functions\n",
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < EPSILON:\n",
    "        return random.randint(0, 3)  # Take a random action\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(state, device=device)\n",
    "            q_values = policy_net(state_tensor)\n",
    "            action = q_values.argmax().item()\n",
    "            return action\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = random.sample(memory, BATCH_SIZE)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    state_batch = torch.tensor(np.array(batch.state), device=device)\n",
    "    action_batch = torch.tensor(batch.action, device=device)\n",
    "    reward_batch = torch.tensor(batch.reward, device=device)\n",
    "\n",
    "    # Handle next_state, replace None with zeros for done states\n",
    "    next_state_batch = np.array(batch.next_state)\n",
    "    next_state_batch = np.nan_to_num(next_state_batch, nan=0)  # Convert None to zeros\n",
    "    next_state_batch = torch.tensor(next_state_batch, device=device)\n",
    "\n",
    "    q_values = policy_net(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "    next_q_values = target_net(next_state_batch).max(1)[0].detach()\n",
    "    expected_q_values = reward_batch + GAMMA * next_q_values\n",
    "\n",
    "    loss = nn.MSELoss()(q_values.squeeze(), expected_q_values)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "total_score = 0 \n",
    "wrong_move_penalty = 0\n",
    "collision_penalty = 0\n",
    "\n",
    "# Main game loop\n",
    "game = SnakeGame()\n",
    "total_wrong_move_penalty = 0  # Track wrong move penalty\n",
    "total_collision_penalty = 0   # Track collision penalty\n",
    "games_played = 0\n",
    "\n",
    "while True:\n",
    "    state = game.reset()\n",
    "    game_over = False\n",
    "\n",
    "    # Reset per-game penalty counters\n",
    "    wrong_move_penalty = 0\n",
    "    collision_penalty = 0\n",
    "\n",
    "    while not game_over:\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                sys.exit()\n",
    "\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, game_over = game.step(action)\n",
    "\n",
    "        # Track penalty types\n",
    "        if reward == -1:\n",
    "            wrong_move_penalty += reward  # Increment wrong move penalty\n",
    "        elif reward == -15:  # Assuming collision penalty is set to -15\n",
    "            collision_penalty += reward  # Increment collision penalty\n",
    "\n",
    "        memory.append(Transition(state, action, next_state, reward))\n",
    "        state = next_state\n",
    "\n",
    "        optimize_model()\n",
    "        game.render()\n",
    "        clock.tick(FPS)\n",
    "\n",
    "    # After each game, accumulate and print penalties\n",
    "    total_wrong_move_penalty += wrong_move_penalty\n",
    "    total_collision_penalty += collision_penalty\n",
    "    total_score += game.score  # Add score of this game to total score\n",
    "\n",
    "    print(f'Game {games_played + 1} ended with score {game.score} wrong moves ({wrong_move_penalty}) colision ({collision_penalty}) .')\n",
    "\n",
    "\n",
    "    if games_played % TARGET_UPDATE_FREQUENCY == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    games_played += 1\n",
    "    EPSILON = max(EPSILON_MIN, EPSILON * EPSILON_DECAY)\n",
    "\n",
    "    if games_played >= 100:\n",
    "        # torch.save(policy_net.state_dict(), \"snake.pt\")\n",
    "        # print(\"Trained model saved as 'snake.pt'\")\n",
    "        print(f'Total Score: {total_score}, Average Score: {total_score / games_played:.2f}')\n",
    "        break\n",
    "\n",
    "pygame.quit()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
